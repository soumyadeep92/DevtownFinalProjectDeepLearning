# -*- coding: utf-8 -*-
"""ANN_WinePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e9Cuhb1QCDmrqyQD8RVwQ13e8-a92pI-

## Wine Quality Prediction

## 1. Importing the libraries
"""

import numpy as np 
import pandas as pd
#import matplotlib.pyplot as plt, seaborn as sns
import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns',None)

"""## 2. Reading the dataset"""

red=pd.read_csv('winequality-red.csv')
print(red.head())

"""## 3. Data Preparation and Cleaning"""

print('Shape of general_data for red wine :',red.shape)
red.head()

red.info()

red.describe()

red.isnull().sum()

"""## 4. Data Analysis and Filtering"""

feature_ = red.columns[:-1]
def get_percentile(feature, q_range):
  dist = red[feature].describe()[str(q_range) + '%']
  return round(dist,2)

"""## 5. Data Visualization"""

#def render_counterplot():
    #fig=plt.figure(figsize=(18, 20))
    #for column, feature in enumerate(feature_):
        #fig.add_subplot(4, 3, column + 1)
        
        #q1 = get_percentile(feature, 25)
        #q2 = get_percentile(feature, 50)
        #q3 = get_percentile(feature, 75)
          
        #sns.histplot(data=red, x=feature, kde=True, color = 'orange')
        
        #plt.axvline(q1, linestyle='--', color='green', label='Q1')
        #plt.axvline(q2, color='red', label='Q2 (Median)')
        #plt.axvline(q3, linestyle='--',  color='black', label='Q3')
        #plt.legend()
        
    #plt.show()
    
#render_counterplot()

#fig, ax = plt.subplots(1, 2)
 
#ax[0].hist(red.alcohol, 10, facecolor ='red', alpha = 0.5, label ="Red wine")
 
 
#fig.subplots_adjust(left = 0, right = 1, bottom = 0,
               #top = 0.5, hspace = 0.05, wspace = 1)
 
#ax[0].set_ylim([0, 1000])
#ax[0].set_xlabel("Alcohol in % Vol")
#ax[0].set_ylabel("Frequency")

 
#fig.suptitle("Distribution of Alcohol in % Vol")
#plt.show()

#plt.figure(figsize = (18,6))
#ax1 = sns.countplot(x='quality', data=red, palette='Set2') 
#plt.xlabel('Red Wine Quality',fontsize  = 14)
#plt.ylabel('Count',fontsize  = 14)
#ax2=ax1.twinx()
#ax2.yaxis.tick_left()
#ax1.yaxis.tick_right()
#ax1.yaxis.set_label_position('right')
#ax2.yaxis.set_label_position('left')

#ax2.set_ylabel('Frequency [%]')
#for p in ax1.patches:
   # x=p.get_bbox().get_points()[:,0]
    #y=p.get_bbox().get_points()[1,1]
    #ax1.annotate('{:.1f}%'.format(100.*y/len(red)), (x.mean(), y), ha='center', va='bottom')

#features_ = red.columns.values[:-1]

#fig=plt.figure(figsize=(16, 26))
#for column, feature in enumerate(features_):
#    if feature != "quality":
#        fig.add_subplot(5, 3, column + 1)
#        sns.boxplot(data=red, x="quality", y=feature, color="#8585f2", palette="bright")
    
#plt.show()

"""## 6. Feature Engineering"""

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state=1, max_depth=12)

x = red.drop(['quality'] , axis = 1)
rf.fit(x, red.quality)
features = red.columns    
importances = rf.feature_importances_    
sorted_index = np.argsort(importances)[:]

#plt.figure(figsize=(18,6))
#plt.title('Feature Importances', fontsize= 14)
#plt.barh(range(len(sorted_index)), importances[sorted_index], color='green', align='center')
#plt.yticks(range(len(sorted_index)), [features[i] for i in sorted_index]) # set x-axis ticks as feature names
#plt.xlabel('Feature Importance', fontsize= 12)
#plt.show()

red.drop(['free sulfur dioxide','citric acid'],axis=1,inplace=True)

X = red.drop('quality', axis = 1)
y = red['quality']

from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,stratify = y, random_state=42)

X

"""## 7. Correlating and normalizing data"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])
X_test[X_train.columns] = scaler.transform(X_test[X_train.columns])

#red_corr = X_train.copy()
#plt.figure(figsize = (18,12))
#sns.heatmap(red_corr.corr(),cbar=True, annot=True) 
#plt.show()

X_train.head()

"""## 8. Applying SMOTE for overfitting"""

from imblearn.over_sampling import SMOTE
smte = SMOTE(random_state=42)
X_train_os, y_train_os = smte.fit_resample(X_train,y_train)
print('Training data classes :5\'s   6\'s   7\'s   4\'s  8\'s 3\'s')
print('Before over-sampling   {}   {}   {}   {}   {}   {}'.format(list(y_train.value_counts().values)[0],list(y_train.value_counts().values)[1],list(y_train.value_counts().values)[2],list(y_train.value_counts().values)[3],list(y_train.value_counts().values)[4],list(y_train.value_counts().values)[5]))
print('After over-sampling    {}   {}   {}   {}  {}  {}\n'.format(list(y_train_os.value_counts().values)[0],list(y_train_os.value_counts().values)[1],list(y_train_os.value_counts().values)[2],list(y_train_os.value_counts().values)[3],list(y_train_os.value_counts().values)[4],list(y_train_os.value_counts().values)[5]))

print('After Over-sampling {} synthetic records were added to the training data and now there are equal proportion of records from each class'.format(len(X_train_os)-len(X_train)))

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train_os = le.fit_transform(y_train_os)
y_test = le.transform(y_test)

y_train_os = pd.DataFrame(y_train_os.reshape(len(y_train_os),1))
y_test = pd.DataFrame(y_test.reshape(len(y_test),1))

import tensorflow as tf                     
y_train_os = tf.keras.utils.to_categorical(y_train_os, 6)
y_test = tf.keras.utils.to_categorical(y_test, 6)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense
from keras.layers import Dropout

"""## 9. Building ANN Model"""

model=Sequential()
model.add(tf.keras.layers.Input(shape = 9,))
model.add(tf.keras.layers.Dense(32,activation='relu'))
model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(6,activation='softmax'))

model.summary()

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics= ['accuracy'])

"""## 10. Iterating the model for more analysis"""

history=model.fit(X_train_os,y_train_os,batch_size=50,epochs=400, verbose=1)

print('Weights :',model.layers[0].get_weights()[0])
print('Bias    :',model.layers[0].get_weights()[1])

"""## 11. Final Prediction and result"""

y_pred = model.predict(X_test)

y_pred.shape

print('Red Wine quality: ',model.predict([[2.34,4.25,2.34,0.23,0.64,1.25,0.24,1.023,2.014]])[0][3])

"""## 12. Final Metrics"""

#_, train_acc = model.evaluate(X_train_os, y_train_os, verbose=0)
#_, test_acc = model.evaluate(X_test, y_test, verbose=0)
#print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
# plot loss during training
#plt.figure(figsize=(15,15))
#plt.subplot(211)
#plt.title('Loss')
#plt.plot(history.history['loss'], label='train')
#plt.legend()
#print()
# plot accuracy during training
#plt.figure(figsize=(15,15))
#plt.subplot(212)
#plt.title('Accuracy')
#plt.plot(history.history['accuracy'], label='train')
#plt.legend()
#plt.show()
